{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.datasets import MNIST\n",
    "from mnist_model import mnist\n",
    "import os\n",
    "\n",
    "if not os.path.exists('./vae_img'):\n",
    "    os.mkdir('./vae_img')\n",
    "mnist_path = './mnist.pth'\n",
    "\n",
    "def to_img(x):\n",
    "    x = 0.5 * (x + 1)\n",
    "    x = x.clamp(0, 1)\n",
    "    x = x.view(x.size(0), 1, 28, 28)\n",
    "    return x\n",
    "\n",
    "\n",
    "num_epochs = 100\n",
    "batch_size = 128\n",
    "learning_rate = 1e-3\n",
    "\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "dataset = MNIST('./data', transform=img_transform, download=True)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(784, 400)\n",
    "        self.fc21 = nn.Linear(400, 20)\n",
    "        self.fc22 = nn.Linear(400, 20)\n",
    "        self.fc3 = nn.Linear(20, 400)\n",
    "        self.fc4 = nn.Linear(400, 784)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparametrize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        eps = torch.FloatTensor(std.size()).normal_()\n",
    "        eps = Variable(eps)\n",
    "        return eps.mul(std).add_(mu)\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return torch.tanh(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparametrize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "model = VAE()\n",
    "\n",
    "reconstruction_function = nn.MSELoss(reduction='sum')\n",
    "\n",
    "\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    \"\"\"\n",
    "    recon_x: generating images\n",
    "    x: origin images\n",
    "    mu: latent mean\n",
    "    logvar: latent log variance\n",
    "    \"\"\"\n",
    "    BCE = reconstruction_function(recon_x, x)  # mse loss\n",
    "    # loss = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD_element = mu.pow(2).add_(logvar.exp()).mul_(-1).add_(1).add_(logvar)\n",
    "    KLD = torch.sum(KLD_element).mul_(-0.5)\n",
    "    # KL divergence\n",
    "    return BCE + KLD\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "mnist = mnist(pretrained=True)\n",
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1320: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/100], loss:31836.5391, Acc:0.767108\n",
      "epoch [2/100], loss:30382.3359, Acc:0.928472\n",
      "epoch [3/100], loss:29679.6797, Acc:0.939088\n",
      "epoch [4/100], loss:29327.3555, Acc:0.944008\n",
      "epoch [5/100], loss:29119.0449, Acc:0.946628\n",
      "epoch [6/100], loss:28985.2734, Acc:0.948316\n",
      "epoch [7/100], loss:28892.8281, Acc:0.950743\n",
      "epoch [8/100], loss:28803.2070, Acc:0.950982\n",
      "epoch [9/100], loss:28585.2637, Acc:0.952320\n",
      "epoch [10/100], loss:28600.0234, Acc:0.952437\n",
      "epoch [11/100], loss:28476.5527, Acc:0.953114\n",
      "epoch [12/100], loss:28329.4590, Acc:0.953808\n",
      "epoch [13/100], loss:28335.5410, Acc:0.954669\n",
      "epoch [14/100], loss:28152.9297, Acc:0.954652\n",
      "epoch [15/100], loss:28227.7637, Acc:0.956218\n",
      "epoch [16/100], loss:28225.1133, Acc:0.956368\n",
      "epoch [17/100], loss:28065.5176, Acc:0.956407\n",
      "epoch [18/100], loss:28020.2793, Acc:0.956673\n",
      "epoch [19/100], loss:28010.5391, Acc:0.956712\n",
      "epoch [20/100], loss:27908.7695, Acc:0.957012\n",
      "epoch [21/100], loss:27889.4082, Acc:0.957362\n",
      "epoch [22/100], loss:27880.1270, Acc:0.958028\n",
      "epoch [23/100], loss:27939.7773, Acc:0.957828\n",
      "epoch [24/100], loss:27768.1953, Acc:0.957945\n",
      "epoch [25/100], loss:27739.8320, Acc:0.957861\n",
      "epoch [26/100], loss:27790.3203, Acc:0.959016\n",
      "epoch [27/100], loss:27687.9766, Acc:0.957717\n",
      "epoch [28/100], loss:27692.3477, Acc:0.957900\n",
      "epoch [29/100], loss:27666.6816, Acc:0.958911\n",
      "epoch [30/100], loss:27691.4336, Acc:0.957684\n",
      "epoch [31/100], loss:27649.0020, Acc:0.959433\n",
      "epoch [32/100], loss:27620.9297, Acc:0.959138\n",
      "epoch [33/100], loss:27605.8691, Acc:0.959111\n",
      "epoch [34/100], loss:27603.7949, Acc:0.959277\n",
      "epoch [35/100], loss:27560.6523, Acc:0.960177\n",
      "epoch [36/100], loss:27616.8398, Acc:0.959982\n",
      "epoch [37/100], loss:27641.7617, Acc:0.958911\n",
      "epoch [38/100], loss:27468.5195, Acc:0.959594\n",
      "epoch [39/100], loss:27486.8047, Acc:0.960132\n",
      "epoch [40/100], loss:27514.0312, Acc:0.959760\n",
      "epoch [41/100], loss:27330.5293, Acc:0.959666\n",
      "epoch [42/100], loss:27442.6992, Acc:0.960243\n",
      "epoch [43/100], loss:27486.9570, Acc:0.960366\n",
      "epoch [44/100], loss:27413.7402, Acc:0.960116\n",
      "epoch [45/100], loss:27453.5176, Acc:0.959533\n",
      "epoch [46/100], loss:27488.2480, Acc:0.960832\n",
      "epoch [47/100], loss:27361.6230, Acc:0.960632\n",
      "epoch [48/100], loss:27326.8242, Acc:0.959288\n",
      "epoch [49/100], loss:27412.9688, Acc:0.960615\n",
      "epoch [50/100], loss:27265.9863, Acc:0.960249\n",
      "epoch [51/100], loss:27381.9688, Acc:0.960466\n",
      "epoch [52/100], loss:27348.8887, Acc:0.961682\n",
      "epoch [53/100], loss:27463.7734, Acc:0.959660\n",
      "epoch [54/100], loss:27326.9180, Acc:0.959749\n",
      "epoch [55/100], loss:27188.8379, Acc:0.959960\n",
      "epoch [56/100], loss:27333.1270, Acc:0.960932\n",
      "epoch [57/100], loss:27245.6660, Acc:0.960815\n",
      "epoch [58/100], loss:27315.9062, Acc:0.960893\n",
      "epoch [59/100], loss:27193.9961, Acc:0.960416\n",
      "epoch [60/100], loss:27384.9746, Acc:0.961232\n",
      "epoch [61/100], loss:27174.5430, Acc:0.961598\n",
      "epoch [62/100], loss:27320.7773, Acc:0.961393\n",
      "epoch [63/100], loss:27190.9609, Acc:0.961076\n",
      "epoch [64/100], loss:27295.3789, Acc:0.960682\n",
      "epoch [65/100], loss:27213.9609, Acc:0.960749\n",
      "epoch [66/100], loss:27385.4707, Acc:0.961665\n",
      "epoch [67/100], loss:27256.7168, Acc:0.961365\n",
      "epoch [68/100], loss:27279.6367, Acc:0.960049\n",
      "epoch [69/100], loss:27182.4141, Acc:0.961387\n",
      "epoch [70/100], loss:27181.1465, Acc:0.960910\n",
      "epoch [71/100], loss:27200.7246, Acc:0.960882\n",
      "epoch [72/100], loss:27232.3516, Acc:0.961776\n",
      "epoch [73/100], loss:27225.9375, Acc:0.960999\n",
      "epoch [74/100], loss:27190.9316, Acc:0.961382\n",
      "epoch [75/100], loss:27173.2930, Acc:0.960360\n",
      "epoch [76/100], loss:27121.6504, Acc:0.961398\n",
      "epoch [77/100], loss:27122.0664, Acc:0.961082\n",
      "epoch [78/100], loss:27189.3770, Acc:0.962081\n",
      "epoch [79/100], loss:27209.6367, Acc:0.960987\n",
      "epoch [80/100], loss:27196.0039, Acc:0.961182\n",
      "epoch [81/100], loss:27170.4375, Acc:0.961848\n",
      "epoch [82/100], loss:27108.0449, Acc:0.962265\n",
      "epoch [83/100], loss:27212.9512, Acc:0.961893\n",
      "epoch [84/100], loss:27104.3379, Acc:0.961765\n",
      "epoch [85/100], loss:27209.4707, Acc:0.961365\n",
      "epoch [86/100], loss:27143.1191, Acc:0.961199\n",
      "epoch [87/100], loss:27070.8457, Acc:0.961665\n",
      "epoch [88/100], loss:27107.5449, Acc:0.961232\n",
      "epoch [89/100], loss:27108.3477, Acc:0.961637\n",
      "epoch [90/100], loss:27104.6738, Acc:0.961260\n",
      "epoch [91/100], loss:27186.1211, Acc:0.961915\n",
      "epoch [92/100], loss:27081.8223, Acc:0.960915\n",
      "epoch [93/100], loss:27142.3125, Acc:0.961765\n",
      "epoch [94/100], loss:27271.4414, Acc:0.962248\n",
      "epoch [95/100], loss:27108.0508, Acc:0.961415\n",
      "epoch [96/100], loss:27170.4219, Acc:0.962631\n",
      "epoch [97/100], loss:27176.9629, Acc:0.961093\n",
      "epoch [98/100], loss:27022.5312, Acc:0.961915\n",
      "epoch [99/100], loss:27140.2734, Acc:0.961593\n",
      "epoch [100/100], loss:27093.4629, Acc:0.961332\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    mnist.eval()\n",
    "    eval_acc = 0.\n",
    "    for data in dataloader:\n",
    "        img, label = data\n",
    "        img = img.view(img.size(0), -1)\n",
    "        img = Variable(img)\n",
    "        # ===================forward=====================\n",
    "        optimizer.zero_grad()\n",
    "        output, mu, logvar = model(img)\n",
    "        loss = loss_function(output, img, mu, logvar)\n",
    "        # ===================backward====================\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # ===================mnist test==================\n",
    "        out = mnist(output)\n",
    "        \n",
    "        _, pred = torch.max(out, 1)\n",
    "        eval_acc += (pred == label).float().mean()\n",
    "    # ===================log========================\n",
    "    print('epoch [{}/{}], loss:{:.4f}, Acc:{:.6f}'\n",
    "          .format(epoch + 1, num_epochs, loss.item(),eval_acc/len(dataloader)))\n",
    "        \n",
    "    if epoch % 5 == 0:\n",
    "        save = to_img(output[0:8].cpu().data)\n",
    "        save_image(save, './vae_img/image_{}.png'.format(epoch))\n",
    "\n",
    "torch.save(model.state_dict(), './vae.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
